{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bpu53nOru08"
      },
      "source": [
        "# ðŸ¥ Lab 3: Medical Image Classification with Neural Networks\n",
        "\n",
        "**AI in Medicine and Healthcare**  \n",
        "**Insper Instituto de Ensino e Pesquisa**  \n",
        "**Week 2 - Class 3**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ Student Information (REQUIRED)\n",
        "\n",
        "**Student 1:**\n",
        "- Name: _______________________\n",
        "- Email: ______________________\n",
        "\n",
        "**Student 2:**\n",
        "- Name: _______________________\n",
        "- Email: ______________________\n",
        "\n",
        "**Date Submitted:** ______________\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Today's Challenge\n",
        "\n",
        "**Can we use the neural networks we learned in Lab 2 to diagnose pneumonia from chest X-rays?**\n",
        "\n",
        "You already know how to:\n",
        "- âœ… Build neural networks\n",
        "- âœ… Implement training loops\n",
        "- âœ… Evaluate with medical metrics\n",
        "\n",
        "**Today's new skills:**\n",
        "- ðŸ†• Load and visualize medical images\n",
        "- ðŸ†• Preprocess images for neural networks\n",
        "- ðŸ†• Apply your Lab 2 knowledge to a new domain\n",
        "\n",
        "**The twist:** We'll use the SAME type of network (MLP) we used for tabular data... on images. Will it work? Let's find out!\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Dataset: Chest X-Ray Pneumonia\n",
        "\n",
        "- **Source:** Kaggle Chest X-Ray Images (Pneumonia)\n",
        "- **Classes:** Normal vs Pneumonia\n",
        "- **Images:** ~5,800 chest X-rays from pediatric patients\n",
        "- **Task:** Binary classification (like Lab 2, but with images!)\n",
        "- **Clinical Importance:** Pneumonia kills ~2.5 million people/year globally\n",
        "\n",
        "---\n",
        "\n",
        "## â±ï¸ Time Allocation\n",
        "\n",
        "- **Part 1:** Image Loading & Visualization (20 min)\n",
        "- **Part 2:** Build MLP Classifier (30 min)\n",
        "- **Part 3:** Training & Evaluation (20 min)\n",
        "- **Part 4:** Reflection & Analysis (20 min)\n",
        "\n",
        "**Total:** 90 minutes\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Grading\n",
        "\n",
        "| Components |\n",
        "|-----------|\n",
        "| Part 1: Image Processing |\n",
        "| Part 2: Model Architecture |\n",
        "| Part 3: Training & Evaluation |\n",
        "| Part 4: Reflection Questions |\n",
        "\n",
        "### Assessment\n",
        "\n",
        "- **Inadequate**: Did not complete all parts\n",
        "- **Adequate**: Completed all parts\n",
        "- **Above Average**: Completed all parts and Bonus Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reRrBitGru09"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸš€ PART 1: Image Loading & Visualization (20 minutes)\n",
        "\n",
        "First, let's see what medical images actually look like!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8sq__Waru0-"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score, precision_score\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p95ApmbHru0-"
      },
      "source": [
        "## Step 1.1: Download the Dataset\n",
        "\n",
        "We'll use a subset of the Chest X-Ray dataset for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPZfE7Rlru0-"
      },
      "outputs": [],
      "source": [
        "# Download and extract dataset\n",
        "!gdown --id 17MSlc-0l_SSRevYBbKwQQ1PQGpi_W6VN\n",
        "!unzip -q chest_xray_compact.zip\n",
        "\n",
        "print(\"âœ“ Dataset ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRsInDTYru0_"
      },
      "source": [
        "## Step 1.2: Explore the Dataset Structure\n",
        "\n",
        "**TODO:** Explore the directory structure and count images in each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU35y3Xqru0_"
      },
      "outputs": [],
      "source": [
        "# TODO: Set the data directory path\n",
        "# Hint: data_dir = Path('chest_xray_compact')\n",
        "\n",
        "data_dir = None\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Count images in train/val/test for both classes\n",
        "# Hint: Use os.listdir() or Path().glob('*.jpeg')\n",
        "# Structure: chest_xray_compact/train/NORMAL/*.jpeg and chest_xray_compact/train/PNEUMONIA/*.jpeg\n",
        "\n",
        "train_normal_count = None\n",
        "train_pneumonia_count = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print summary\n",
        "print(\"Dataset Structure:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Training set:\")\n",
        "print(f\"  Normal: {train_normal_count} images\")\n",
        "print(f\"  Pneumonia: {train_pneumonia_count} images\")\n",
        "print(f\"  Total: {train_normal_count + train_pneumonia_count} images\")\n",
        "print(f\"\\nClass balance: {train_pneumonia_count/(train_normal_count + train_pneumonia_count)*100:.1f}% pneumonia\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0Caddojru0_"
      },
      "source": [
        "## Step 1.3: Visualize Sample Images\n",
        "\n",
        "**TODO:** Load and display sample chest X-rays from both classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEhV6na0ru0_"
      },
      "outputs": [],
      "source": [
        "# TODO: Load one normal and one pneumonia X-ray\n",
        "# Hint: Use PIL.Image.open(image_path)\n",
        "# Paths: chest_xray_compact/train/NORMAL/... and chest_xray_compact/train/PNEUMONIA/...\n",
        "\n",
        "# Get sample image paths\n",
        "normal_images = None  # list(Path('chest_xray_compact/train/NORMAL').glob('*.jpeg'))\n",
        "pneumonia_images = None  # list(Path('chest_xray_compact/train/PNEUMONIA').glob('*.jpeg'))\n",
        "\n",
        "\n",
        "\n",
        "# Load first image from each class\n",
        "normal_img = None  # Image.open(normal_images[0])\n",
        "pneumonia_img = None  # Image.open(pneumonia_images[0])\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Display the images side by side\n",
        "# Hint: Use matplotlib subplot\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot normal\n",
        "# axes[0].imshow(normal_img, cmap='gray')\n",
        "# axes[0].set_title('Normal Chest X-Ray', fontsize=14, fontweight='bold')\n",
        "# axes[0].axis('off')\n",
        "\n",
        "\n",
        "\n",
        "# Plot pneumonia\n",
        "# axes[1].imshow(pneumonia_img, cmap='gray')\n",
        "# axes[1].set_title('Pneumonia Chest X-Ray', fontsize=14, fontweight='bold')\n",
        "# axes[1].axis('off')\n",
        "\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print image information\n",
        "print(f\"Normal X-ray size: {normal_img.size if normal_img else 'N/A'}\")\n",
        "print(f\"Pneumonia X-ray size: {pneumonia_img.size if pneumonia_img else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DqcRoJnru0_"
      },
      "source": [
        "## Step 1.4: Create Custom Dataset Class\n",
        "\n",
        "**TODO:** Create a PyTorch Dataset class to load images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbjloQd8ru1A"
      },
      "outputs": [],
      "source": [
        "# TODO: Define a custom Dataset class\n",
        "# Hint: Inherit from torch.utils.data.Dataset\n",
        "# Must implement __init__, __len__, and __getitem__\n",
        "\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: Directory with 'NORMAL' and 'PNEUMONIA' subdirectories\n",
        "            transform: Optional transform to apply to images\n",
        "        \"\"\"\n",
        "        # TODO: Store parameters\n",
        "        # self.root_dir = root_dir\n",
        "        # self.transform = transform\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Collect all image paths and labels\n",
        "        # Hint: Loop through NORMAL (label=0) and PNEUMONIA (label=1) folders\n",
        "        # Store as list of tuples: [(path1, label1), (path2, label2), ...]\n",
        "\n",
        "        self.images = []  # Will store (image_path, label) tuples\n",
        "\n",
        "        # Normal images (label = 0)\n",
        "\n",
        "\n",
        "\n",
        "        # Pneumonia images (label = 1)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO: Return total number of images\n",
        "\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO: Load image and label at index idx\n",
        "        # Hint:\n",
        "        # 1. Get image path and label from self.images[idx]\n",
        "        # 2. Load image using PIL.Image.open()\n",
        "        # 3. Convert to RGB if needed\n",
        "        # 4. Apply transform if provided\n",
        "        # 5. Return (image_tensor, label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        pass\n",
        "\n",
        "# Test the dataset\n",
        "print(\"Testing Dataset class...\")\n",
        "# Uncomment when implemented:\n",
        "# test_dataset = ChestXRayDataset('chest_xray_compact/train')\n",
        "# print(f\"Total images: {len(test_dataset)}\")\n",
        "# sample_img, sample_label = test_dataset[0]\n",
        "# print(f\"Sample image type: {type(sample_img)}\")\n",
        "# print(f\"Sample label: {sample_label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xLZDrfAru1A"
      },
      "source": [
        "## Step 1.5: Define Image Transforms\n",
        "\n",
        "**TODO:** Create transforms to preprocess images for the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afByPRFMru1A"
      },
      "outputs": [],
      "source": [
        "# TODO: Define image transformations\n",
        "# Required steps:\n",
        "# 1. Resize to fixed size (e.g., 64x64 for faster training)\n",
        "# 2. Convert to grayscale\n",
        "# 3. Convert to tensor\n",
        "# 4. Normalize (mean=0.5, std=0.5 for grayscale)\n",
        "\n",
        "# Hint: Use transforms.Compose([\n",
        "#     transforms.Resize((64, 64)),\n",
        "#     transforms.Grayscale(num_output_channels=1),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "# ])\n",
        "\n",
        "transform = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Image transformations defined:\")\n",
        "print(transform if transform else \"TODO: Define transforms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8eQADD1ru1A"
      },
      "source": [
        "## Step 1.6: Create DataLoaders\n",
        "\n",
        "**TODO:** Create train, validation, and test DataLoaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByQfi3SPru1A"
      },
      "outputs": [],
      "source": [
        "# TODO: Create datasets with transforms\n",
        "# Hint: train_dataset = ChestXRayDataset('chest_xray_compact/train', transform=transform)\n",
        "\n",
        "train_dataset = None\n",
        "val_dataset = None\n",
        "test_dataset = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Create DataLoaders\n",
        "# Hint: Use batch_size=32, shuffle=True for train\n",
        "\n",
        "train_loader = None\n",
        "val_loader = None\n",
        "test_loader = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Verify\n",
        "print(\"DataLoaders created:\")\n",
        "print(f\"Train batches: {len(train_loader) if train_loader else 'N/A'}\")\n",
        "print(f\"Val batches: {len(val_loader) if val_loader else 'N/A'}\")\n",
        "print(f\"Test batches: {len(test_loader) if test_loader else 'N/A'}\")\n",
        "\n",
        "# Test loading a batch\n",
        "if train_loader:\n",
        "    images, labels = next(iter(train_loader))\n",
        "    print(f\"\\nBatch shape: {images.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "    print(f\"Image range: [{images.min():.2f}, {images.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2sRHEKLru1A"
      },
      "source": [
        "## ðŸŽ‰ Part 1 Complete!\n",
        "\n",
        "**Checkpoint:**\n",
        "- âœ… Dataset downloaded and explored\n",
        "- âœ… Sample images visualized\n",
        "- âœ… Custom Dataset class created\n",
        "- âœ… Image transforms defined\n",
        "- âœ… DataLoaders ready\n",
        "\n",
        "**Key Insight:** Images are 64Ã—64 pixels = 4,096 input features!\n",
        "(Compare to Lab 2: only 8 features)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4FgmhjAru1A"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸ§  PART 2: Build MLP Classifier (30 minutes)\n",
        "\n",
        "**The Challenge:** Use the SAME type of network from Lab 2 (MLP) on images.\n",
        "\n",
        "**Key Question:** A 64Ã—64 grayscale image has 4,096 pixels. How do we feed this to an MLP?\n",
        "\n",
        "**Answer:** FLATTEN the image into a 1D vector!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m167FrkLru1B"
      },
      "source": [
        "## Step 2.1: Visualize What Flattening Does\n",
        "\n",
        "**TODO:** Show what happens when we flatten a 2D image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRWe0td0ru1B"
      },
      "outputs": [],
      "source": [
        "# Get a sample image\n",
        "if train_loader:\n",
        "    sample_images, sample_labels = next(iter(train_loader))\n",
        "    sample_img = sample_images[0]  # Shape: (1, 64, 64)\n",
        "\n",
        "    # TODO: Flatten the image\n",
        "    # Hint: flattened = sample_img.view(-1)\n",
        "\n",
        "    flattened = None\n",
        "\n",
        "\n",
        "\n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "    # Original image\n",
        "    axes[0].imshow(sample_img.squeeze(), cmap='gray')\n",
        "    axes[0].set_title('Original Image\\n(64Ã—64 = 4,096 pixels)', fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Flattened visualization\n",
        "    if flattened is not None:\n",
        "        axes[1].plot(flattened.numpy())\n",
        "        axes[1].set_title('Flattened to 1D Vector\\n(4,096 values)', fontweight='bold')\n",
        "        axes[1].set_xlabel('Pixel index')\n",
        "        axes[1].set_ylabel('Pixel value')\n",
        "\n",
        "    # Show what's lost\n",
        "    axes[2].text(0.5, 0.5, 'âŒ Spatial Information\\nLOST!\\n\\n'\n",
        "                 'Pixels that were neighbors\\nare now far apart in the vector',\n",
        "                 ha='center', va='center', fontsize=14,\n",
        "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Original shape: {sample_img.shape}\")\n",
        "    print(f\"Flattened shape: {flattened.shape if flattened is not None else 'N/A'}\")\n",
        "    print(f\"\\nðŸ¤” Think about: Does the network know which pixels were next to each other?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9HgbKcmru1B"
      },
      "source": [
        "## Step 2.2: Design the MLP Architecture\n",
        "\n",
        "**TODO:** Create an MLP classifier for images.\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "Input: 4,096 pixels (flattened 64Ã—64 image)\n",
        "Hidden 1: 128 neurons + ReLU\n",
        "Hidden 2: 64 neurons + ReLU\n",
        "Output: 2 classes (Normal vs Pneumonia)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvHL9Pv2ru1B"
      },
      "outputs": [],
      "source": [
        "# TODO: Define the MLP classifier\n",
        "# This is VERY similar to Lab 2, but with different input size!\n",
        "\n",
        "class ChestXRayMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ChestXRayMLP, self).__init__()\n",
        "\n",
        "        # TODO: Define layers\n",
        "        # Input size: 64*64 = 4096\n",
        "        # Hint: self.fc1 = nn.Linear(4096, 128)\n",
        "        #       self.relu = nn.ReLU()\n",
        "        #       self.fc2 = nn.Linear(128, 64)\n",
        "        #       self.fc3 = nn.Linear(64, 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement forward pass\n",
        "        # IMPORTANT: First flatten the image!\n",
        "        # Hint: x = x.view(x.size(0), -1)  # Flatten to (batch_size, 4096)\n",
        "        #       x = self.fc1(x)\n",
        "        #       x = self.relu(x)\n",
        "        #       ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        pass\n",
        "\n",
        "# TODO: Create model instance\n",
        "model = None  # ChestXRayMLP()\n",
        "\n",
        "\n",
        "\n",
        "# Print model info\n",
        "if model:\n",
        "    print(\"Model Architecture:\")\n",
        "    print(\"=\"*60)\n",
        "    print(model)\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "    print(f\"Compare to Lab 2: ~300 parameters\")\n",
        "    print(f\"This model is {total_params/300:.0f}Ã— larger!\")\n",
        "\n",
        "    # Test forward pass\n",
        "    test_input = torch.randn(1, 1, 64, 64)\n",
        "    test_output = model(test_input)\n",
        "    print(f\"\\nTest: Input {test_input.shape} â†’ Output {test_output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMoxcDNMru1B"
      },
      "source": [
        "## ðŸŽ‰ Part 2 Complete!\n",
        "\n",
        "**Checkpoint:**\n",
        "- âœ… Understand image flattening\n",
        "- âœ… MLP architecture defined\n",
        "- âœ… MUCH larger than Lab 2 (due to 4,096 inputs!)\n",
        "\n",
        "**Think about:** Why does flattening seem problematic?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xld5lBlzru1B"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸ”„ PART 3: Training & Evaluation (20 minutes)\n",
        "\n",
        "**This should look VERY familiar from Lab 2!**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neQk0S5Iru1B"
      },
      "source": [
        "## Step 3.1: Training Setup\n",
        "\n",
        "**TODO:** Set up loss, optimizer, and device (same as Lab 2!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-m7u55Jru1B"
      },
      "outputs": [],
      "source": [
        "# TODO: Define loss function\n",
        "criterion = None  # nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# TODO: Define optimizer\n",
        "optimizer = None  # optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# TODO: Set device\n",
        "device = None  # torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# TODO: Move model to device\n",
        "# model = model.to(device)\n",
        "\n",
        "\n",
        "# Set epochs\n",
        "num_epochs = 10  # Fewer epochs since images are slower to train\n",
        "\n",
        "print(\"Training Setup:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Loss: {criterion}\")\n",
        "print(f\"Optimizer: {optimizer.__class__.__name__ if optimizer else 'N/A'}\")\n",
        "print(f\"Epochs: {num_epochs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiCA9fBvru1C"
      },
      "source": [
        "## Step 3.2: Training Loop\n",
        "\n",
        "**TODO:** Implement training loop (copy from Lab 2 and adapt!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmhXuBharu1C"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement training loop\n",
        "# This is IDENTICAL to Lab 2 - you can reuse that code!\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # TODO: Training phase\n",
        "    # Hint: Same as Lab 2!\n",
        "    # 1. model.train()\n",
        "    # 2. Loop through train_loader\n",
        "    # 3. optimizer.zero_grad()\n",
        "    # 4. Forward pass\n",
        "    # 5. Compute loss\n",
        "    # 6. loss.backward()\n",
        "    # 7. optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: Validation phase\n",
        "    # Hint: Same as Lab 2!\n",
        "    # 1. model.eval()\n",
        "    # 2. with torch.no_grad()\n",
        "    # 3. Loop through val_loader\n",
        "    # 4. Compute accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    pass\n",
        "\n",
        "print(\"âœ“ Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NO7QDbMru1C"
      },
      "source": [
        "## Step 3.3: Plot Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p77txi9Mru1C"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot training curves (same as Lab 2!)\n",
        "\n",
        "if len(train_losses) > 0:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss curves\n",
        "    ax1.plot(train_losses, label='Training Loss', linewidth=2)\n",
        "    ax1.plot(val_losses, label='Validation Loss', linewidth=2)\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training Progress')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy curve\n",
        "    ax2.plot(val_accuracies, label='Validation Accuracy', color='green', linewidth=2)\n",
        "    ax2.axhline(y=50, color='red', linestyle='--', label='Random Guess')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.set_title('Validation Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLw815w5ru1C"
      },
      "source": [
        "## Step 3.4: Evaluate on Test Set\n",
        "\n",
        "**TODO:** Compute medical metrics on test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vwm5lkBru1C"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate on test set (same as Lab 2!)\n",
        "# Compute: accuracy, precision, sensitivity, specificity, confusion matrix\n",
        "\n",
        "if model:\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            # TODO: Get predictions\n",
        "\n",
        "\n",
        "\n",
        "            pass\n",
        "\n",
        "    # TODO: Compute metrics\n",
        "    # accuracy = accuracy_score(all_labels, all_preds)\n",
        "    # precision = precision_score(all_labels, all_preds)\n",
        "    # sensitivity = recall_score(all_labels, all_preds)\n",
        "    # cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nTest Set Results:\")\n",
        "    print(\"=\"*50)\n",
        "    # print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "    # print(f\"Precision: {precision*100:.2f}%\")\n",
        "    # print(f\"Sensitivity: {sensitivity*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFU1Crwsru1C"
      },
      "source": [
        "## ðŸŽ‰ Part 3 Complete!\n",
        "\n",
        "**How did we do?**\n",
        "\n",
        "Compare to Lab 2:\n",
        "- Lab 2 (diabetes, tabular): ~76% accuracy\n",
        "- Lab 3 (pneumonia, images): ~??% accuracy\n",
        "\n",
        "**Interesting, right?** ðŸ¤”\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtGGUUWRru1C"
      },
      "source": [
        "---\n",
        "\n",
        "# ðŸ¤” PART 4: Reflection & Analysis (20 minutes)\n",
        "\n",
        "**The most important part: THINKING about what we learned!**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYu3hA5vru1D"
      },
      "source": [
        "## Reflection Question 1: Performance Comparison\n",
        "\n",
        "**Compare your results:**\n",
        "\n",
        "| Metric | Lab 2 (Diabetes) | Lab 3 (Pneumonia) | Difference |\n",
        "|--------|------------------|-------------------|------------|\n",
        "| Input Features | 8 | 4,096 | 512Ã— more |\n",
        "| Parameters | ~300 | ~500,000 | 1,600Ã— more |\n",
        "| Accuracy | ~76% | ??% | ??% |\n",
        "| Training Time/Epoch | Fast | Slower | ?? |\n",
        "\n",
        "**Question:** Why might we get WORSE accuracy despite having MORE features and a BIGGER model?\n",
        "\n",
        "**Your answer:**\n",
        "\n",
        "_[Write your thoughts here]_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSJ0HeIFru1D"
      },
      "source": [
        "## Reflection Question 2: The Flattening Problem\n",
        "\n",
        "When we flatten a 64Ã—64 image:\n",
        "\n",
        "```\n",
        "Before:        After:\n",
        "[row 0]        [pixel 0, pixel 1, pixel 2, ..., pixel 4095]\n",
        "[row 1]        \n",
        "[row 2]        Pixels that were neighbors\n",
        " ...           are now far apart!\n",
        "[row 63]\n",
        "```\n",
        "\n",
        "**Questions to think about:**\n",
        "\n",
        "1. **Does the MLP know that pixel 0 and pixel 1 were next to each other?**\n",
        "   - Your answer: _[Yes/No and why]_\n",
        "\n",
        "2. **In a chest X-ray, what matters more: individual pixel values OR patterns of nearby pixels?**\n",
        "   - Your answer: _[Individual values / Patterns / Both?]_\n",
        "\n",
        "3. **Example:** The pattern for \"lung opacity\" (pneumonia indicator) involves a cluster of bright pixels. Does our MLP naturally detect clusters?\n",
        "   - Your answer: _[Yes/No and why]_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygn4Mky8ru1D"
      },
      "source": [
        "## Reflection Question 3: What's Missing?\n",
        "\n",
        "**When a radiologist looks at a chest X-ray, they look for:**\n",
        "- Shapes (heart silhouette, lung boundaries)\n",
        "- Textures (smooth vs. patchy)\n",
        "- Relative positions (is the opacity in the upper or lower lobe?)\n",
        "- Edges (clear vs. fuzzy boundaries)\n",
        "\n",
        "**Question:** Our MLP treats each pixel independently. What medical information might we be losing?\n",
        "\n",
        "**Your answer:**\n",
        "\n",
        "_[Write specific examples of spatial information that flattening destroys]_\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpyTFy9wru1D"
      },
      "source": [
        "## Reflection Question 4: If You Were Designing a Better Architecture...\n",
        "\n",
        "**Imagine you could design a network specifically for images. What would you want it to do differently?**\n",
        "\n",
        "Some ideas to consider:\n",
        "- Process nearby pixels together?\n",
        "- Look for local patterns (edges, shapes)?\n",
        "- Understand that shifting the image slightly doesn't change the diagnosis?\n",
        "- Build up understanding from small patterns â†’ larger patterns?\n",
        "\n",
        "**Your ideas:**\n",
        "\n",
        "_[Write your thoughts on what would make a better image classifier]_\n",
        "\n",
        "**Hint:** We'll explore solutions to these problems in the coming weeks! ðŸ˜‰\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43ANV38gru1D"
      },
      "source": [
        "## Bonus Exploration: Visualize What the Model Learned\n",
        "\n",
        "**TODO (Optional):** Look at some misclassified examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw7Qi6-qru1D"
      },
      "outputs": [],
      "source": [
        "# Optional: Show examples of correct and incorrect predictions\n",
        "# This helps understand where the model struggles\n",
        "\n",
        "# TODO: Get predictions and find misclassified images\n",
        "# Show side-by-side: correct predictions vs wrong predictions\n",
        "\n",
        "# Your exploration code here (optional):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4yFbKkkru1O"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ“ Lab 3 Summary\n",
        "\n",
        "### What We Accomplished:\n",
        "\n",
        "âœ… **Applied Lab 2 knowledge** to a new domain (images!)  \n",
        "âœ… **Loaded and visualized** medical images  \n",
        "âœ… **Built an MLP** for image classification  \n",
        "âœ… **Trained and evaluated** with medical metrics  \n",
        "âœ… **Discovered limitations** of MLPs for images  \n",
        "\n",
        "### Key Insights:\n",
        "\n",
        "1. **Flattening loses spatial information** - crucial for images!\n",
        "2. **More features â‰  better performance** - if architecture is wrong\n",
        "3. **Images are different from tabular data** - need different approaches\n",
        "4. **Medical imaging is challenging** - but solvable with right tools!\n",
        "\n",
        "### Coming Up:\n",
        "\n",
        "- **Next week:** DICOM protocol, medical imaging modalities\n",
        "- **Soon:** Better architectures for images (hint: CNNs!)\n",
        "- **2-3 weeks:** Hospital visit to see MRI/CT machines!\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“¤ Submission Instructions\n",
        "\n",
        "1. **Complete all TODOs** in this notebook\n",
        "2. **Answer reflection questions** thoughtfully\n",
        "3. **Save:** File â†’ Download â†’ .ipynb\n",
        "4. **Name:** `Lab3_LastName1_LastName2.ipynb`\n",
        "5. **Submit to Blackboard** by [deadline]\n",
        "\n",
        "**Questions to discuss next class!** ðŸ’­\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}