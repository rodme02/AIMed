{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB4ZCozfYOMh"
      },
      "source": [
        "# Week 1, Class 2: Introduction to Machine Learning in Healthcare\n",
        "## Hands-on Lab: PyTorch Fundamentals & Your First Neural Network\n",
        "\n",
        "**Course:** AI/ML in Medicine and Healthcare  \n",
        "**Module:** Week 1 - Foundations  \n",
        "\n",
        "---\n",
        "\n",
        "## üìù Student Information\n",
        "\n",
        "**Student 1:**\n",
        "- Name: Rodrigo Paoliello de Medeiros\n",
        "- Email: rodrigopm6@al.insper.edu.br\n",
        "\n",
        "**Student 2:**\n",
        "- Name: Gabriel Hermida Mendon√ßa\n",
        "- Email: gabrielmmh@al.insper.edu.br\n",
        "\n",
        "**Date Submitted:** 12/02/2026\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this lab, you will:\n",
        "1. ‚úÖ Master essential PyTorch tensor operations\n",
        "2. ‚úÖ **Build a complete neural network from scratch**\n",
        "3. ‚úÖ **Implement the full training loop** (forward, loss, backward, optimize)\n",
        "4. ‚úÖ **Evaluate with medical metrics** (sensitivity, specificity)\n",
        "5. ‚úÖ **Experiment with architectures** and hyperparameters\n",
        "\n",
        "---\n",
        "\n",
        "## ‚è±Ô∏è Time Allocation\n",
        "\n",
        "- **Part 1:** Tensor Operations (30 minutes)\n",
        "- **Part 2:** Neural Network (60 minutes)\n",
        "  - Milestones 1-5: ~50 minutes\n",
        "  - Milestone 6 (Experimentation): ~30+ minutes\n",
        "\n",
        "**Total:** 90 minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJrclUjXYOMi"
      },
      "source": [
        "---\n",
        "\n",
        "# üöÄ PART 1: Tensor Operations Speed Run (30 minutes)\n",
        "\n",
        "## Instructions:\n",
        "\n",
        "Complete the following challenges. If you're **experienced with PyTorch**, this should take 15-20 minutes. If you're **new to PyTorch**, take your time and ask questions!\n",
        "\n",
        "**Format:** Each challenge is independent. Complete as many as you can.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C99-HEDkYOMj",
        "outputId": "0325c173-5d0f-4b86-8b70-e811e6c2a82c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu128\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgShJQcYYOMj"
      },
      "source": [
        "## Challenge 1: Create Medical Data Tensors (5 minutes)\n",
        "\n",
        "**Task:** Create a tensor representing **100 patients** with **8 medical features** each.\n",
        "\n",
        "Features:\n",
        "1. Age (years): 20-80\n",
        "2. BMI (kg/m¬≤): 18-40\n",
        "3. Blood Pressure (mmHg): 80-180\n",
        "4. Glucose (mg/dL): 70-200\n",
        "5. Insulin (ŒºU/mL): 0-200\n",
        "6. Pregnancies: 0-15\n",
        "7. Skin Thickness (mm): 0-99\n",
        "8. Diabetes Pedigree Function: 0.0-2.5\n",
        "\n",
        "**Requirements:**\n",
        "- Use realistic value ranges for each feature\n",
        "- Final tensor shape: `(100, 8)`\n",
        "- Data type: `float32`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1OtS4C33YOMj",
        "outputId": "40ccdce5-88d9-4580-b6b2-194d56f75dac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "You need to create the patients_data tensor!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3066270100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Verification (don't modify)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mpatients_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You need to create the patients_data tensor!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mpatients_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Expected shape (100, 8), got {patients_data.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mpatients_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Expected dtype float32, got {patients_data.dtype}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: You need to create the patients_data tensor!"
          ]
        }
      ],
      "source": [
        "# TODO: Create a tensor with 100 patients and 8 features\n",
        "# Hint: You can use torch.randn() and scale/shift to appropriate ranges\n",
        "# Or create individual feature columns and concatenate\n",
        "\n",
        "# Your code here:\n",
        "# Define value ranges for each feature\n",
        "# Age (years): 20-80\n",
        "# BMI (kg/m¬≤): 18-40\n",
        "# Blood Pressure (mmHg): 80-180\n",
        "# Glucose (mg/dL): 70-200\n",
        "# Insulin (ŒºU/mL): 0-200\n",
        "# Pregnancies: 0-15\n",
        "# Skin Thickness (mm): 0-99\n",
        "# Diabetes Pedigree Function: 0.0-2.5\n",
        "\n",
        "patients_data = torch.cat([\n",
        "    (torch.rand(100, 1) * 60) + 20,    # Age: 20-80\n",
        "    (torch.rand(100, 1) * 22) + 18,    # BMI: 18-40\n",
        "    (torch.rand(100, 1) * 100) + 80,   # Blood Pressure: 80-180\n",
        "    (torch.rand(100, 1) * 130) + 70,   # Glucose: 70-200\n",
        "    (torch.rand(100, 1) * 200),        # Insulin: 0-200\n",
        "    (torch.rand(100, 1) * 15).floor(), # Pregnancies: 0-15 (integer)\n",
        "    (torch.rand(100, 1) * 99),         # Skin Thickness: 0-99\n",
        "    (torch.rand(100, 1) * 2.5)         # Diabetes Pedigree Function: 0.0-2.5\n",
        "], dim=1).to(torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "# Verification (don't modify)\n",
        "assert patients_data is not None, \"You need to create the patients_data tensor!\"\n",
        "assert patients_data.shape == (100, 8), f\"Expected shape (100, 8), got {patients_data.shape}\"\n",
        "assert patients_data.dtype == torch.float32, f\"Expected dtype float32, got {patients_data.dtype}\"\n",
        "print(\"‚úì Challenge 1 complete!\")\n",
        "print(f\"Tensor shape: {patients_data.shape}\")\n",
        "print(f\"Sample patient data:\\n{patients_data[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RY5rgPRYOMk"
      },
      "source": [
        "## Challenge 2: One-Hot Encoding (5 minutes)\n",
        "\n",
        "**Task:** Convert diagnosis labels to one-hot encoded vectors.\n",
        "\n",
        "Given: `labels = [0, 1, 1, 0, 1]` (0 = no diabetes, 1 = diabetes)\n",
        "\n",
        "**Expected output:**\n",
        "```\n",
        "[[1, 0],  # Patient 0: no diabetes\n",
        " [0, 1],  # Patient 1: diabetes\n",
        " [0, 1],  # Patient 2: diabetes\n",
        " [1, 0],  # Patient 3: no diabetes\n",
        " [0, 1]]  # Patient 4: diabetes\n",
        "```\n",
        "\n",
        "**Hint:** Use `torch.nn.functional.one_hot()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MAs2XKEYOMk"
      },
      "outputs": [],
      "source": [
        "# Given labels\n",
        "labels = torch.tensor([0, 1, 1, 0, 1])\n",
        "\n",
        "# TODO: Convert to one-hot encoding\n",
        "# Hint: import torch.nn.functional as F\n",
        "#       one_hot = F.one_hot(labels, num_classes=2)\n",
        "\n",
        "one_hot_labels = None  # Replace with your code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Verification (don't modify)\n",
        "assert one_hot_labels is not None, \"You need to create one_hot_labels!\"\n",
        "assert one_hot_labels.shape == (5, 2), f\"Expected shape (5, 2), got {one_hot_labels.shape}\"\n",
        "assert torch.equal(one_hot_labels[0], torch.tensor([1, 0])), \"First patient encoding incorrect\"\n",
        "print(\"‚úì Challenge 2 complete!\")\n",
        "print(f\"One-hot encoded labels:\\n{one_hot_labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz0Z5ip3YOMk"
      },
      "source": [
        "## Challenge 3: Feature Normalization (5 minutes)\n",
        "\n",
        "**Task:** Normalize the patient data to have **mean=0** and **standard deviation=1** for each feature.\n",
        "\n",
        "**Formula:** `normalized = (x - mean) / std`\n",
        "\n",
        "**Requirements:**\n",
        "- Normalize each of the 8 features independently\n",
        "- Use the `patients_data` tensor from Challenge 1\n",
        "- Verify: mean ‚âà 0, std ‚âà 1 for each feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SYhSayVYOMl"
      },
      "outputs": [],
      "source": [
        "# TODO: Normalize patients_data\n",
        "# Hint: check numpy or pandas API for mean and stdev functions\n",
        "\n",
        "normalized_data = None  # Replace with your code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Verification (don't modify)\n",
        "assert normalized_data is not None, \"You need to create normalized_data!\"\n",
        "assert normalized_data.shape == patients_data.shape, \"Shape should not change\"\n",
        "\n",
        "# Check normalization\n",
        "means = normalized_data.mean(dim=0)\n",
        "stds = normalized_data.std(dim=0)\n",
        "print(\"‚úì Challenge 3 complete!\")\n",
        "print(f\"Mean per feature (should be ~0): {means}\")\n",
        "print(f\"Std per feature (should be ~1): {stds}\")\n",
        "assert torch.allclose(means, torch.zeros(8), atol=1e-6), \"Mean should be close to 0\"\n",
        "assert torch.allclose(stds, torch.ones(8), atol=1e-1), \"Std should be close to 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiVTxHFjYOMl"
      },
      "source": [
        "## Challenge 4: Batch Operations (5 minutes)\n",
        "\n",
        "**Task:** Split 1000 patients into batches of 32 using PyTorch's DataLoader.\n",
        "\n",
        "**Requirements:**\n",
        "1. Create synthetic data for 1000 patients with 8 features\n",
        "2. Create corresponding labels (0 or 1)\n",
        "3. Create a `TensorDataset`\n",
        "4. Create a `DataLoader` with batch_size=32, shuffle=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_xtfwWLYOMl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# TODO: Create data for 1000 patients\n",
        "X = None  # Shape: (1000, 8)\n",
        "y = None  # Shape: (1000,)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Create TensorDataset\n",
        "dataset = None  # TensorDataset(X, y)\n",
        "\n",
        "\n",
        "# TODO: Create DataLoader with batch_size=32\n",
        "dataloader = None\n",
        "\n",
        "\n",
        "\n",
        "# Verification (don't modify)\n",
        "assert X is not None and y is not None, \"Create X and y!\"\n",
        "assert X.shape == (1000, 8), f\"Expected X shape (1000, 8), got {X.shape}\"\n",
        "assert y.shape == (1000,), f\"Expected y shape (1000,), got {y.shape}\"\n",
        "assert dataloader is not None, \"Create the dataloader!\"\n",
        "\n",
        "# Test the dataloader\n",
        "first_batch = next(iter(dataloader))\n",
        "print(\"‚úì Challenge 4 complete!\")\n",
        "print(f\"Number of batches: {len(dataloader)}\")\n",
        "print(f\"First batch X shape: {first_batch[0].shape}\")\n",
        "print(f\"First batch y shape: {first_batch[1].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEzHGv8aYOMl"
      },
      "source": [
        "## Challenge 5: GPU Transfer (5 minutes)\n",
        "\n",
        "**Task:** Move tensors between CPU and GPU (if available).\n",
        "\n",
        "**Requirements:**\n",
        "1. Check if CUDA is available\n",
        "2. Create a device variable (cuda or cpu)\n",
        "3. Move a tensor to the device\n",
        "4. Perform an operation on the device\n",
        "5. Move the result back to CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J0lftlhYOMl"
      },
      "outputs": [],
      "source": [
        "# TODO: Set device to GPU if available, else CPU\n",
        "device = None  # torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Create a sample tensor\n",
        "sample_tensor = torch.randn(100, 8)\n",
        "\n",
        "# TODO: Move tensor to device\n",
        "tensor_on_device = None\n",
        "\n",
        "\n",
        "# TODO: Perform operation on device (e.g., multiply by 2)\n",
        "result_on_device = None\n",
        "\n",
        "\n",
        "# TODO: Move result back to CPU\n",
        "result_cpu = None\n",
        "\n",
        "\n",
        "\n",
        "# Verification (don't modify)\n",
        "assert device is not None, \"Set the device!\"\n",
        "assert tensor_on_device is not None, \"Move tensor to device!\"\n",
        "assert result_cpu is not None, \"Move result to CPU!\"\n",
        "print(\"‚úì Challenge 5 complete!\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Tensor on device: {tensor_on_device.device}\")\n",
        "print(f\"Result on CPU: {result_cpu.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5wTaLhNYOMl"
      },
      "source": [
        "---\n",
        "\n",
        "# üß† PART 2: Build Your First Neural Network (60 minutes)\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "Build a **neural network to diagnose diabetes** from patient medical data.\n",
        "\n",
        "**Dataset:** Pima Indians Diabetes Dataset (familiar from Lab 1!)\n",
        "- 768 patients\n",
        "- 8 features: pregnancies, glucose, blood pressure, skin thickness, insulin, BMI, diabetes pedigree, age\n",
        "- Target: Diabetes diagnosis (0 = no, 1 = yes)\n",
        "\n",
        "**Your Task:**\n",
        "1. Load and prepare the data\n",
        "2. Define a neural network architecture\n",
        "3. **Implement the training loop from scratch** ‚Üê This is the most important part!\n",
        "4. Evaluate with medical metrics\n",
        "5. Experiment and optimize\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRcb16ZKYOMl"
      },
      "source": [
        "## üìä Milestone 1: Data Preparation (10 minutes)\n",
        "\n",
        "**Objectives:**\n",
        "1. Load the Pima diabetes dataset\n",
        "2. Split into features (X) and labels (y)\n",
        "3. Convert to PyTorch tensors\n",
        "4. Split into train/validation/test sets (60/20/20)\n",
        "5. Create DataLoaders with batch_size=32\n",
        "\n",
        "**Hints provided, but YOU write the code!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUodJPX7YOMm"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!wget -q https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv -O diabetes.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeTnDBKIYOMm"
      },
      "outputs": [],
      "source": [
        "# Import additional libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# TODO: Load the data\n",
        "# Hint: df = pd.read_csv('diabetes.csv', header=None)\n",
        "#       Column names: ['Pregnancies','Glucose','BloodPressure','SkinThickness',\n",
        "#                      'Insulin','BMI','DiabetesPedigree','Age','Outcome']\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Separate features (X) and labels (y)\n",
        "# Hint: X = df.iloc[:, :-1].values  # All columns except last\n",
        "#       y = df.iloc[:, -1].values   # Last column only\n",
        "\n",
        "X = None\n",
        "y = None\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Split into train/val/test (60/20/20)\n",
        "# Hint: First split into train (60%) and temp (40%)\n",
        "#       Then split temp into val (50% of temp = 20% overall) and test (50% of temp = 20% overall)\n",
        "#       Use stratify=y to maintain class balance\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = None, None, None, None\n",
        "X_val, X_test, y_val, y_test = None, None, None, None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Normalize the features (using training set statistics)\n",
        "# Hint: scaler = StandardScaler()\n",
        "#       X_train_scaled = scaler.fit_transform(X_train)\n",
        "#       X_val_scaled = scaler.transform(X_val)\n",
        "#       X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Convert to PyTorch tensors\n",
        "# Hint: X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "#       y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "X_train_tensor = None\n",
        "y_train_tensor = None\n",
        "X_val_tensor = None\n",
        "y_val_tensor = None\n",
        "X_test_tensor = None\n",
        "y_test_tensor = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Create TensorDatasets\n",
        "# Hint: train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "\n",
        "train_dataset = None\n",
        "val_dataset = None\n",
        "test_dataset = None\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Create DataLoaders with batch_size=32\n",
        "# Hint: train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "train_loader = None\n",
        "val_loader = None\n",
        "test_loader = None\n",
        "\n",
        "\n",
        "\n",
        "# ‚úì CHECKPOINT: Verify shapes\n",
        "print(\"Data Preparation Complete!\")\n",
        "print(f\"Train set: {X_train_tensor.shape[0]} samples\")\n",
        "print(f\"Val set: {X_val_tensor.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test_tensor.shape[0]} samples\")\n",
        "print(f\"Features: {X_train_tensor.shape[1]}\")\n",
        "print(f\"Batches in train_loader: {len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiexsrMLYOMm"
      },
      "source": [
        "## üèóÔ∏è Milestone 2: Define the Neural Network (15 minutes)\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "Input Layer:    8 features\n",
        "Hidden Layer 1: 16 neurons + ReLU activation\n",
        "Hidden Layer 2: 8 neurons + ReLU activation\n",
        "Output Layer:   2 neurons (no diabetes / diabetes)\n",
        "```\n",
        "\n",
        "**Your Task:** Build this network using `nn.Module`\n",
        "\n",
        "**Key Concepts:**\n",
        "- `nn.Linear(in_features, out_features)` - Fully connected layer\n",
        "- `nn.ReLU()` - Activation function\n",
        "- `forward()` - Define how data flows through the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifPAsmPzYOMm"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# TODO: Define the neural network class\n",
        "class DiabetesClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesClassifier, self).__init__()\n",
        "\n",
        "        # TODO: Define layers\n",
        "        # Hint: self.fc1 = nn.Linear(8, 16)  # Input to hidden1\n",
        "        #       self.relu = nn.ReLU()\n",
        "        #       self.fc2 = nn.Linear(16, 8)  # Hidden1 to hidden2\n",
        "        #       self.fc3 = nn.Linear(8, 2)   # Hidden2 to output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement forward pass\n",
        "        # Hint: x = self.fc1(x)\n",
        "        #       x = self.relu(x)\n",
        "        #       x = self.fc2(x)\n",
        "        #       x = self.relu(x)\n",
        "        #       x = self.fc3(x)\n",
        "        #       return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        pass\n",
        "\n",
        "# TODO: Instantiate the model\n",
        "model = None  # DiabetesClassifier()\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Print model architecture\n",
        "# print(model)\n",
        "\n",
        "\n",
        "# ‚úì CHECKPOINT: Count parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "if model is not None:\n",
        "    print(f\"\\nTotal parameters: {count_parameters(model)}\")\n",
        "    print(\"Expected: ~300 parameters\")\n",
        "\n",
        "    # Test forward pass\n",
        "    test_input = torch.randn(1, 8)\n",
        "    test_output = model(test_input)\n",
        "    print(f\"\\nTest input shape: {test_input.shape}\")\n",
        "    print(f\"Test output shape: {test_output.shape}\")\n",
        "    print(\"Expected output shape: torch.Size([1, 2])\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55iTgII0YOMm"
      },
      "source": [
        "## ‚öôÔ∏è Milestone 3: Training Setup (10 minutes)\n",
        "\n",
        "**Components needed:**\n",
        "1. Loss function - `CrossEntropyLoss` (for classification)\n",
        "2. Optimizer - `Adam` (adaptive learning rate)\n",
        "3. Learning rate - Start with 0.001\n",
        "4. Device - GPU if available, CPU otherwise\n",
        "5. Number of epochs - 50\n",
        "\n",
        "**Why these choices?**\n",
        "- CrossEntropyLoss: Standard for multi-class classification\n",
        "- Adam: Generally works well, adapts learning rate automatically\n",
        "- lr=0.001: Good starting point, can tune later\n",
        "- 50 epochs: Enough to see convergence, not too slow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvc0gnoIYOMm"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# TODO: Define loss function\n",
        "# Hint: criterion = nn.CrossEntropyLoss()\n",
        "criterion = None\n",
        "\n",
        "\n",
        "# TODO: Define optimizer\n",
        "# Hint: optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer = None\n",
        "\n",
        "\n",
        "# TODO: Set device (GPU if available)\n",
        "# Hint: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = None\n",
        "\n",
        "\n",
        "# TODO: Move model to device\n",
        "# Hint: model = model.to(device)\n",
        "\n",
        "\n",
        "# Set number of epochs\n",
        "num_epochs = 50\n",
        "\n",
        "# ‚úì CHECKPOINT: Verify setup\n",
        "print(\"Training Setup Complete!\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Loss function: {criterion}\")\n",
        "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
        "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "print(f\"Number of epochs: {num_epochs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC5gCicLYOMm"
      },
      "source": [
        "## üîÑ Milestone 4: Training Loop - THE MOST IMPORTANT! (15 minutes)\n",
        "\n",
        "**This is where the learning happens!**\n",
        "\n",
        "For each epoch:\n",
        "1. **Training Phase:**\n",
        "   - Set model to training mode\n",
        "   - For each batch:\n",
        "     - Move batch to device\n",
        "     - Zero gradients (IMPORTANT!)\n",
        "     - Forward pass\n",
        "     - Compute loss\n",
        "     - Backward pass (compute gradients)\n",
        "     - Update weights\n",
        "\n",
        "2. **Validation Phase:**\n",
        "   - Set model to evaluation mode\n",
        "   - No gradient computation\n",
        "   - Compute validation loss and accuracy\n",
        "\n",
        "**This is the CORE of deep learning - understand every line!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbYG01WXYOMm"
      },
      "outputs": [],
      "source": [
        "# Lists to store metrics\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ============================================\n",
        "    # TRAINING PHASE\n",
        "    # ============================================\n",
        "\n",
        "    # TODO: Set model to training mode\n",
        "    # Hint: model.train()\n",
        "\n",
        "\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # TODO: Move batch to device\n",
        "        # Hint: batch_X = batch_X.to(device)\n",
        "        #       batch_y = batch_y.to(device)\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Zero the gradients (CRITICAL!)\n",
        "        # Hint: optimizer.zero_grad()\n",
        "        # Why? Gradients accumulate by default - we need to clear them each iteration\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Forward pass\n",
        "        # Hint: outputs = model(batch_X)\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Compute loss\n",
        "        # Hint: loss = criterion(outputs, batch_y)\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Backward pass (compute gradients)\n",
        "        # Hint: loss.backward()\n",
        "        # This is backpropagation!\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Update weights\n",
        "        # Hint: optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        # Accumulate loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # TODO: Calculate average training loss\n",
        "    # Hint: train_loss = train_loss / len(train_loader)\n",
        "\n",
        "\n",
        "\n",
        "    # ============================================\n",
        "    # VALIDATION PHASE\n",
        "    # ============================================\n",
        "\n",
        "    # TODO: Set model to evaluation mode\n",
        "    # Hint: model.eval()\n",
        "\n",
        "\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # TODO: Disable gradient computation (saves memory and computation)\n",
        "    # Hint: with torch.no_grad():\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            # TODO: Move batch to device\n",
        "\n",
        "\n",
        "\n",
        "            # TODO: Forward pass\n",
        "\n",
        "\n",
        "\n",
        "            # TODO: Compute loss\n",
        "\n",
        "\n",
        "\n",
        "            # Accumulate loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # TODO: Compute accuracy\n",
        "            # Hint: _, predicted = torch.max(outputs, 1)\n",
        "            #       total += batch_y.size(0)\n",
        "            #       correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: Calculate average validation loss and accuracy\n",
        "    # Hint: val_loss = val_loss / len(val_loader)\n",
        "    #       val_accuracy = 100 * correct / total\n",
        "\n",
        "\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print progress every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "        print(f\"  Val Accuracy: {val_accuracy:.2f}%\\n\")\n",
        "\n",
        "print(\"‚úì Training Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ62MAYfYOMn"
      },
      "source": [
        "### üìà Visualize Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjNaYcC4YOMn"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curves\n",
        "ax1.plot(train_losses, label='Training Loss', linewidth=2)\n",
        "ax1.plot(val_losses, label='Validation Loss', linewidth=2)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curve\n",
        "ax2.plot(val_accuracies, label='Validation Accuracy', color='green', linewidth=2)\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.set_title('Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracies[-1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsQjplNoYOMn"
      },
      "source": [
        "## üìä Milestone 5: Medical Evaluation Metrics (10 minutes)\n",
        "\n",
        "**In medical diagnosis, accuracy is NOT enough!**\n",
        "\n",
        "We need:\n",
        "- **Sensitivity (Recall)** - How many actual diabetics did we catch?\n",
        "- **Specificity** - How many non-diabetics did we correctly identify?\n",
        "- **Precision** - Of those we diagnosed as diabetic, how many actually are?\n",
        "- **F1-Score** - Harmonic mean of precision and recall\n",
        "\n",
        "**Why this matters:**\n",
        "- Missing a diabetic patient (False Negative) can be life-threatening\n",
        "- Falsely diagnosing diabetes (False Positive) causes unnecessary treatment\n",
        "- We need to balance these based on clinical priorities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbIdoi65YOMn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# TODO: Set model to evaluation mode\n",
        "# Hint: model.eval()\n",
        "\n",
        "\n",
        "# Get predictions on test set\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        # TODO: Move to device\n",
        "\n",
        "\n",
        "        # TODO: Get predictions\n",
        "        # Hint: outputs = model(batch_X)\n",
        "        #       _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# TODO: Compute metrics\n",
        "# Hint: accuracy = accuracy_score(all_labels, all_predictions)\n",
        "#       precision = precision_score(all_labels, all_predictions)\n",
        "#       sensitivity = recall_score(all_labels, all_predictions)  # Sensitivity = Recall\n",
        "#       f1 = f1_score(all_labels, all_predictions)\n",
        "\n",
        "accuracy = None\n",
        "precision = None\n",
        "sensitivity = None  # This is Recall\n",
        "f1 = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Compute specificity manually\n",
        "# Hint: cm = confusion_matrix(all_labels, all_predictions)\n",
        "#       tn, fp, fn, tp = cm.ravel()\n",
        "#       specificity = tn / (tn + fp)\n",
        "\n",
        "cm = None\n",
        "specificity = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MEDICAL EVALUATION METRICS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:    {accuracy*100:.2f}%\")\n",
        "print(f\"Precision:   {precision*100:.2f}%  (Of predicted diabetic, how many are correct?)\")\n",
        "print(f\"Sensitivity: {sensitivity*100:.2f}%  (Of actual diabetic, how many did we catch?)\")\n",
        "print(f\"Specificity: {specificity*100:.2f}%  (Of actual non-diabetic, how many correct?)\")\n",
        "print(f\"F1-Score:    {f1:.4f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Baseline comparison\n",
        "baseline_accuracy = max(np.mean(all_labels), 1 - np.mean(all_labels))\n",
        "print(f\"\\nBaseline (always predict majority class): {baseline_accuracy*100:.2f}%\")\n",
        "print(f\"Our model improvement: +{(accuracy - baseline_accuracy)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpY0gz79YOMn"
      },
      "source": [
        "### üéØ Confusion Matrix Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iqKiN6AYOMn"
      },
      "outputs": [],
      "source": [
        "# TODO: Create confusion matrix heatmap\n",
        "# Hint: Use seaborn heatmap\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Diabetes', 'Diabetes'],\n",
        "            yticklabels=['No Diabetes', 'Diabetes'])\n",
        "\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Confusion Matrix\\n')\n",
        "\n",
        "# Add interpretation\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "plt.text(0.5, -0.15, f\"True Negatives: {tn}\", transform=plt.gca().transAxes, ha='center')\n",
        "plt.text(0.5, -0.20, f\"False Positives: {fp} (unnecessary treatment)\", transform=plt.gca().transAxes, ha='center')\n",
        "plt.text(0.5, -0.25, f\"False Negatives: {fn} (missed diagnoses - CRITICAL!)\", transform=plt.gca().transAxes, ha='center', color='red')\n",
        "plt.text(0.5, -0.30, f\"True Positives: {tp}\", transform=plt.gca().transAxes, ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nClinical Interpretation:\")\n",
        "print(f\"- We correctly identified {tp} diabetic patients\")\n",
        "print(f\"- We MISSED {fn} diabetic patients (this is bad!)\")\n",
        "print(f\"- We falsely diagnosed {fp} healthy patients (unnecessary worry/treatment)\")\n",
        "print(f\"- We correctly identified {tn} healthy patients\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATHhpWGUYOMn"
      },
      "source": [
        "### üìã Detailed Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncxxgjj_YOMn"
      },
      "outputs": [],
      "source": [
        "# Print detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(all_labels, all_predictions,\n",
        "                          target_names=['No Diabetes', 'Diabetes']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH1kpTmsYOMn"
      },
      "source": [
        "## üî¨ Milestone 6: Experimentation & Optimization (Remaining time)\n",
        "\n",
        "**Now it's YOUR turn to improve the model!**\n",
        "\n",
        "Try different approaches and document what works:\n",
        "\n",
        "### Ideas to Experiment With:\n",
        "\n",
        "1. **Architecture Changes:**\n",
        "   - More/fewer layers\n",
        "   - Different layer sizes\n",
        "   - Add dropout for regularization\n",
        "   - Add batch normalization\n",
        "\n",
        "2. **Training Changes:**\n",
        "   - Different learning rates (0.0001, 0.01)\n",
        "   - Different optimizers (SGD, RMSprop)\n",
        "   - Different batch sizes (16, 64)\n",
        "   - More/fewer epochs\n",
        "\n",
        "3. **Data Changes:**\n",
        "   - Different train/val/test splits\n",
        "   - Different normalization\n",
        "   - Handle class imbalance (weighted loss)\n",
        "\n",
        "**Keep track of your experiments!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_2q_TPWYOMn"
      },
      "source": [
        "### Experiment 1: Deeper Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISWFE00LYOMn"
      },
      "outputs": [],
      "source": [
        "# TODO: Try a deeper network\n",
        "# Example: 8 ‚Üí 32 ‚Üí 16 ‚Üí 8 ‚Üí 2\n",
        "\n",
        "class DeeperClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeeperClassifier, self).__init__()\n",
        "        # Your architecture here\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your forward pass here\n",
        "        pass\n",
        "\n",
        "# Train and evaluate\n",
        "# (Copy training loop from above and modify)\n",
        "\n",
        "# Document results:\n",
        "# Accuracy: _____\n",
        "# Sensitivity: _____\n",
        "# Better than original? _____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-bZo9ExYOMn"
      },
      "source": [
        "### Experiment 2: Add Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPp0xtc1YOMo"
      },
      "outputs": [],
      "source": [
        "# TODO: Add dropout layers (e.g., 0.2 or 0.5)\n",
        "# Dropout helps prevent overfitting\n",
        "\n",
        "class DropoutClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DropoutClassifier, self).__init__()\n",
        "        # Add dropout: self.dropout = nn.Dropout(0.2)\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply dropout between layers\n",
        "        pass\n",
        "\n",
        "# Document results:\n",
        "# Accuracy: _____\n",
        "# Did it reduce overfitting? _____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rxrcDs8YOMo"
      },
      "source": [
        "### Experiment 3: Different Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3VnRuvuYOMo"
      },
      "outputs": [],
      "source": [
        "# TODO: Try lr=0.0001 (lower) or lr=0.01 (higher)\n",
        "\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Document results:\n",
        "# Learning rate: _____\n",
        "# Convergence speed: _____\n",
        "# Final accuracy: _____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvirruXtYOMo"
      },
      "source": [
        "### Experiment 4: Class Imbalance Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6l_S5ysYOMs"
      },
      "outputs": [],
      "source": [
        "# TODO: Use weighted loss to handle class imbalance\n",
        "# Medical datasets often have imbalanced classes\n",
        "\n",
        "# Calculate class weights\n",
        "# class_counts = np.bincount(y_train)\n",
        "# class_weights = 1. / class_counts\n",
        "# weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "# criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "# Document results:\n",
        "# Did sensitivity improve? _____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4voUEC3wYOMt"
      },
      "source": [
        "### üìä Experiment Comparison Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ1K3yrpYOMt"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a comparison table of all your experiments\n",
        "\n",
        "results = {\n",
        "    'Experiment': ['Baseline', 'Deeper', 'Dropout', 'Low LR', 'Weighted Loss'],\n",
        "    'Accuracy': [None, None, None, None, None],\n",
        "    'Sensitivity': [None, None, None, None, None],\n",
        "    'Specificity': [None, None, None, None, None],\n",
        "    'F1-Score': [None, None, None, None, None]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "# Which experiment performed best?\n",
        "# Your analysis:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XOqjFk_YOMt"
      },
      "source": [
        "---\n",
        "\n",
        "## üéì Reflection Questions\n",
        "\n",
        "Answer these questions based on your experiments:\n",
        "\n",
        "1. **What was the biggest challenge in building the neural network?**\n",
        "   - Your answer:\n",
        "\n",
        "2. **Which component of the training loop did you find most confusing initially?**\n",
        "   - Your answer:\n",
        "\n",
        "3. **Why is sensitivity (recall) particularly important for medical diagnosis?**\n",
        "   - Your answer:\n",
        "\n",
        "4. **What was your best performing model configuration?**\n",
        "   - Your answer:\n",
        "\n",
        "5. **If you were deploying this in a real hospital, what additional considerations would you have?**\n",
        "   - Your answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJWJv78NYOMt"
      },
      "source": [
        "## üéâ Lab 2 Complete!\n",
        "\n",
        "### What You Accomplished:\n",
        "\n",
        "‚úÖ Mastered PyTorch tensor operations  \n",
        "‚úÖ Built a complete neural network from scratch  \n",
        "‚úÖ Implemented the full training loop  \n",
        "‚úÖ Evaluated with medical metrics  \n",
        "‚úÖ Experimented with improvements  \n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "- **The training loop is fundamental** - forward, loss, backward, optimize\n",
        "- **Medical metrics matter** - accuracy alone is insufficient\n",
        "- **Experimentation is essential** - no \"perfect\" hyperparameters\n",
        "- **Neural networks are powerful** but require careful tuning\n",
        "\n",
        "**Good work! üéì‚ú®**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}