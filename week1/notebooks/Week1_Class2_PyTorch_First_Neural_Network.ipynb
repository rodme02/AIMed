{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB4ZCozfYOMh"
      },
      "source": [
        "# Week 1, Class 2: Introduction to Machine Learning in Healthcare\n",
        "## Hands-on Lab: PyTorch Fundamentals & Your First Neural Network\n",
        "\n",
        "**Course:** AI/ML in Medicine and Healthcare  \n",
        "**Module:** Week 1 - Foundations  \n",
        "\n",
        "---\n",
        "\n",
        "## üìù Student Information\n",
        "\n",
        "**Student 1:**\n",
        "- Name: Rodrigo Paoliello de Medeiros\n",
        "- Email: rodrigopm6@al.insper.edu.br\n",
        "\n",
        "**Student 2:**\n",
        "- Name: Gabriel Hermida Mendon√ßa\n",
        "- Email: gabrielmmh@al.insper.edu.br\n",
        "\n",
        "**Date Submitted:** 12/02/2026\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this lab, you will:\n",
        "1. ‚úÖ Master essential PyTorch tensor operations\n",
        "2. ‚úÖ **Build a complete neural network from scratch**\n",
        "3. ‚úÖ **Implement the full training loop** (forward, loss, backward, optimize)\n",
        "4. ‚úÖ **Evaluate with medical metrics** (sensitivity, specificity)\n",
        "5. ‚úÖ **Experiment with architectures** and hyperparameters\n",
        "\n",
        "---\n",
        "\n",
        "## ‚è±Ô∏è Time Allocation\n",
        "\n",
        "- **Part 1:** Tensor Operations (30 minutes)\n",
        "- **Part 2:** Neural Network (60 minutes)\n",
        "  - Milestones 1-5: ~50 minutes\n",
        "  - Milestone 6 (Experimentation): ~30+ minutes\n",
        "\n",
        "**Total:** 90 minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJrclUjXYOMi"
      },
      "source": [
        "---\n",
        "\n",
        "# üöÄ PART 1: Tensor Operations Speed Run (30 minutes)\n",
        "\n",
        "## Instructions:\n",
        "\n",
        "Complete the following challenges. If you're **experienced with PyTorch**, this should take 15-20 minutes. If you're **new to PyTorch**, take your time and ask questions!\n",
        "\n",
        "**Format:** Each challenge is independent. Complete as many as you can.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C99-HEDkYOMj",
        "outputId": "65239ae0-4022-4489-8a89-6900f2ac4a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu128\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgShJQcYYOMj"
      },
      "source": [
        "## Challenge 1: Create Medical Data Tensors (5 minutes)\n",
        "\n",
        "**Task:** Create a tensor representing **100 patients** with **8 medical features** each.\n",
        "\n",
        "Features:\n",
        "1. Age (years): 20-80\n",
        "2. BMI (kg/m¬≤): 18-40\n",
        "3. Blood Pressure (mmHg): 80-180\n",
        "4. Glucose (mg/dL): 70-200\n",
        "5. Insulin (ŒºU/mL): 0-200\n",
        "6. Pregnancies: 0-15\n",
        "7. Skin Thickness (mm): 0-99\n",
        "8. Diabetes Pedigree Function: 0.0-2.5\n",
        "\n",
        "**Requirements:**\n",
        "- Use realistic value ranges for each feature\n",
        "- Final tensor shape: `(100, 8)`\n",
        "- Data type: `float32`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OtS4C33YOMj",
        "outputId": "d06f3f89-1640-400a-e336-de3e569ca2d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Challenge 1 complete!\n",
            "Tensor shape: torch.Size([100, 8])\n",
            "Sample patient data:\n",
            "tensor([ 28.9723,  28.3285, 138.1012,  93.5830,  51.8733,  14.0000,  17.9821,\n",
            "          1.6716])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a tensor with 100 patients and 8 features\n",
        "# Hint: You can use torch.randn() and scale/shift to appropriate ranges\n",
        "# Or create individual feature columns and concatenate\n",
        "\n",
        "num_patients = 100\n",
        "\n",
        "patients_data = torch.cat([\n",
        "    torch.clamp(torch.randn(num_patients, 1) * 15 + 50, min=20, max=80),    # Age: mean=50, std=15 (approx 95% in 20-80)\n",
        "    torch.clamp(torch.randn(num_patients, 1) * 5.5 + 29, min=18, max=40),    # BMI: mean=29, std=5.5 (approx 95% in 18-40)\n",
        "    torch.clamp(torch.randn(num_patients, 1) * 25 + 130, min=80, max=180),   # Blood Pressure: mean=130, std=25\n",
        "    torch.clamp(torch.randn(num_patients, 1) * 32.5 + 135, min=70, max=200), # Glucose: mean=135, std=32.5\n",
        "    torch.clamp(torch.randn(num_patients, 1) * 50 + 100, min=0, max=200),    # Insulin: mean=100, std=50\n",
        "    torch.clamp(torch.randn(num_patients, 1) * 3.75 + 7.5, min=0, max=15).round(), # Pregnancies: mean=7.5, std=3.75 (integer)\n",
        "    torch.clamp(torch.randn(num_patients, 1) * 24.75 + 49.5, min=0, max=99), # Skin Thickness: mean=49.5, std=24.75\n",
        "    torch.clamp(torch.randn(num_patients, 1) * 0.625 + 1.25, min=0.0, max=2.5) # Diabetes Pedigree Function: mean=1.25, std=0.625\n",
        "], dim=1).to(torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "# Verification (don't modify)\n",
        "assert patients_data is not None, \"You need to create the patients_data tensor!\"\n",
        "assert patients_data.shape == (100, 8), f\"Expected shape (100, 8), got {patients_data.shape}\"\n",
        "assert patients_data.dtype == torch.float32, f\"Expected dtype float32, got {patients_data.dtype}\"\n",
        "print(\"‚úì Challenge 1 complete!\")\n",
        "print(f\"Tensor shape: {patients_data.shape}\")\n",
        "print(f\"Sample patient data:\\n{patients_data[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RY5rgPRYOMk"
      },
      "source": [
        "## Challenge 2: One-Hot Encoding (5 minutes)\n",
        "\n",
        "**Task:** Convert diagnosis labels to one-hot encoded vectors.\n",
        "\n",
        "Given: `labels = [0, 1, 1, 0, 1]` (0 = no diabetes, 1 = diabetes)\n",
        "\n",
        "**Expected output:**\n",
        "```\n",
        "[[1, 0],  # Patient 0: no diabetes\n",
        " [0, 1],  # Patient 1: diabetes\n",
        " [0, 1],  # Patient 2: diabetes\n",
        " [1, 0],  # Patient 3: no diabetes\n",
        " [0, 1]]  # Patient 4: diabetes\n",
        "```\n",
        "\n",
        "**Hint:** Use `torch.nn.functional.one_hot()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0MAs2XKEYOMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65bf9ad-76b2-4f91-d8ed-2ea290a57ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Challenge 2 complete!\n",
            "One-hot encoded labels:\n",
            "tensor([[1, 0],\n",
            "        [0, 1],\n",
            "        [0, 1],\n",
            "        [1, 0],\n",
            "        [0, 1]])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Given labels\n",
        "labels = torch.tensor([0, 1, 1, 0, 1])\n",
        "\n",
        "\n",
        "one_hot_labels = F.one_hot(labels, num_classes=2)\n",
        "\n",
        "\n",
        "# Verification (don't modify)\n",
        "assert one_hot_labels is not None, \"You need to create one_hot_labels!\"\n",
        "assert one_hot_labels.shape == (5, 2), f\"Expected shape (5, 2), got {one_hot_labels.shape}\"\n",
        "assert torch.equal(one_hot_labels[0], torch.tensor([1, 0])), \"First patient encoding incorrect\"\n",
        "print(\"‚úì Challenge 2 complete!\")\n",
        "print(f\"One-hot encoded labels:\\n{one_hot_labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz0Z5ip3YOMk"
      },
      "source": [
        "## Challenge 3: Feature Normalization (5 minutes)\n",
        "\n",
        "**Task:** Normalize the patient data to have **mean=0** and **standard deviation=1** for each feature.\n",
        "\n",
        "**Formula:** `normalized = (x - mean) / std`\n",
        "\n",
        "**Requirements:**\n",
        "- Normalize each of the 8 features independently\n",
        "- Use the `patients_data` tensor from Challenge 1\n",
        "- Verify: mean ‚âà 0, std ‚âà 1 for each feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0SYhSayVYOMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a202bf70-2320-4b9b-bc08-aa7c5f6f8020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Challenge 3 complete!\n",
            "Mean per feature (should be ~0): tensor([-3.8147e-08,  1.1921e-07, -3.4094e-07, -9.0599e-08, -5.5432e-08,\n",
            "        -4.2915e-08, -1.6689e-08, -8.3447e-08])\n",
            "Std per feature (should be ~1): tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Normalize patients_data\n",
        "# Hint: check numpy or pandas API for mean and stdev functions\n",
        "\n",
        "# Calculate mean and standard deviation for each feature\n",
        "mean = patients_data.mean(dim=0)\n",
        "std = patients_data.std(dim=0)\n",
        "\n",
        "# Apply normalization\n",
        "normalized_data = (patients_data - mean) / std\n",
        "\n",
        "# Verification (don't modify)\n",
        "assert normalized_data is not None, \"You need to create normalized_data!\"\n",
        "assert normalized_data.shape == patients_data.shape, \"Shape should not change\"\n",
        "\n",
        "# Check normalization\n",
        "means = normalized_data.mean(dim=0)\n",
        "stds = normalized_data.std(dim=0)\n",
        "print(\"‚úì Challenge 3 complete!\")\n",
        "print(f\"Mean per feature (should be ~0): {means}\")\n",
        "print(f\"Std per feature (should be ~1): {stds}\")\n",
        "assert torch.allclose(means, torch.zeros(8), atol=1e-6), \"Mean should be close to 0\"\n",
        "assert torch.allclose(stds, torch.ones(8), atol=1e-1), \"Std should be close to 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiVTxHFjYOMl"
      },
      "source": [
        "## Challenge 4: Batch Operations (5 minutes)\n",
        "\n",
        "**Task:** Split 1000 patients into batches of 32 using PyTorch's DataLoader.\n",
        "\n",
        "**Requirements:**\n",
        "1. Create synthetic data for 1000 patients with 8 features\n",
        "2. Create corresponding labels (0 or 1)\n",
        "3. Create a `TensorDataset`\n",
        "4. Create a `DataLoader` with batch_size=32, shuffle=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m_xtfwWLYOMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14a9fa8-85b2-4d1d-bdab-e3f54a136e0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Challenge 4 complete!\n",
            "Number of batches: 32\n",
            "First batch X shape: torch.Size([32, 8])\n",
            "First batch y shape: torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# TODO: Create data for 1000 patients\n",
        "X = torch.randn(1000, 8)  # Shape: (1000, 8)\n",
        "y = torch.randint(0, 2, (1000,)) # Shape: (1000,) - 0 or 1 labels\n",
        "\n",
        "# TODO: Create TensorDataset\n",
        "dataset = TensorDataset(X, y)  # TensorDataset(X, y)\n",
        "\n",
        "# TODO: Create DataLoader with batch_size=32\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Verification (don't modify)\n",
        "assert X is not None and y is not None, \"Create X and y!\"\n",
        "assert X.shape == (1000, 8), f\"Expected X shape (1000, 8), got {X.shape}\"\n",
        "assert y.shape == (1000,), f\"Expected y shape (1000,), got {y.shape}\"\n",
        "assert dataloader is not None, \"Create the dataloader!\"\n",
        "\n",
        "# Test the dataloader\n",
        "first_batch = next(iter(dataloader))\n",
        "print(\"‚úì Challenge 4 complete!\")\n",
        "print(f\"Number of batches: {len(dataloader)}\")\n",
        "print(f\"First batch X shape: {first_batch[0].shape}\")\n",
        "print(f\"First batch y shape: {first_batch[1].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEzHGv8aYOMl"
      },
      "source": [
        "## Challenge 5: GPU Transfer (5 minutes)\n",
        "\n",
        "**Task:** Move tensors between CPU and GPU (if available).\n",
        "\n",
        "**Requirements:**\n",
        "1. Check if CUDA is available\n",
        "2. Create a device variable (cuda or cpu)\n",
        "3. Move a tensor to the device\n",
        "4. Perform an operation on the device\n",
        "5. Move the result back to CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-J0lftlhYOMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b290ec0b-9290-424e-e19d-3d596107add0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Challenge 5 complete!\n",
            "Device: cuda\n",
            "Tensor on device: cuda:0\n",
            "Result on CPU: cpu\n"
          ]
        }
      ],
      "source": [
        "# TODO: Set device to GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Create a sample tensor\n",
        "sample_tensor = torch.randn(100, 8)\n",
        "\n",
        "# TODO: Move tensor to device\n",
        "tensor_on_device = sample_tensor.to(device)\n",
        "\n",
        "\n",
        "# TODO: Perform operation on device (e.g., multiply by 2)\n",
        "result_on_device = tensor_on_device * 2\n",
        "\n",
        "\n",
        "# TODO: Move result back to CPU\n",
        "result_cpu = result_on_device.cpu()\n",
        "\n",
        "\n",
        "\n",
        "# Verification (don't modify)\n",
        "assert device is not None, \"Set the device!\"\n",
        "assert tensor_on_device is not None, \"Move tensor to device!\"\n",
        "assert result_cpu is not None, \"Move result to CPU!\"\n",
        "print(\"‚úì Challenge 5 complete!\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Tensor on device: {tensor_on_device.device}\")\n",
        "print(f\"Result on CPU: {result_cpu.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5wTaLhNYOMl"
      },
      "source": [
        "---\n",
        "\n",
        "# üß† PART 2: Build Your First Neural Network (60 minutes)\n",
        "\n",
        "## The Challenge\n",
        "\n",
        "Build a **neural network to diagnose diabetes** from patient medical data.\n",
        "\n",
        "**Dataset:** Pima Indians Diabetes Dataset (familiar from Lab 1!)\n",
        "- 768 patients\n",
        "- 8 features: pregnancies, glucose, blood pressure, skin thickness, insulin, BMI, diabetes pedigree, age\n",
        "- Target: Diabetes diagnosis (0 = no, 1 = yes)\n",
        "\n",
        "**Your Task:**\n",
        "1. Load and prepare the data\n",
        "2. Define a neural network architecture\n",
        "3. **Implement the training loop from scratch** ‚Üê This is the most important part!\n",
        "4. Evaluate with medical metrics\n",
        "5. Experiment and optimize\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRcb16ZKYOMl"
      },
      "source": [
        "## üìä Milestone 1: Data Preparation (10 minutes)\n",
        "\n",
        "**Objectives:**\n",
        "1. Load the Pima diabetes dataset\n",
        "2. Split into features (X) and labels (y)\n",
        "3. Convert to PyTorch tensors\n",
        "4. Split into train/validation/test sets (60/20/20)\n",
        "5. Create DataLoaders with batch_size=32\n",
        "\n",
        "**Hints provided, but YOU write the code!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUodJPX7YOMm"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!wget -q https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv -O diabetes.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UeTnDBKIYOMm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "6ee2ff63-2d11-4824-97f2-14df5f39c94e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'diabetes.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3980253047.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#       Column names: ['Pregnancies','Glucose','BloodPressure','SkinThickness',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#                      'Insulin','BMI','DiabetesPedigree','Age','Outcome']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'diabetes.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'diabetes.csv'"
          ]
        }
      ],
      "source": [
        "# Import additional libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# TODO: Load the data\n",
        "# Hint: df = pd.read_csv('diabetes.csv', header=None)\n",
        "#       Column names: ['Pregnancies','Glucose','BloodPressure','SkinThickness',\n",
        "#                      'Insulin','BMI','DiabetesPedigree','Age','Outcome']\n",
        "df = pd.read_csv('diabetes.csv', header=None)\n",
        "\n",
        "\n",
        "# TODO: Separate features (X) and labels (y)\n",
        "# Hint: X = df.iloc[:, :-1].values  # All columns except last\n",
        "#       y = df.iloc[:, -1].values   # Last column only\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Split into train/val/test (60/20/20)\n",
        "# Hint: First split into train (60%) and temp (40%)\n",
        "#       Then split temp into val (50% of temp = 20% overall) and test (50% of temp = 20% overall)\n",
        "#       Use stratify=y to maintain class balance\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Normalize the features (using training set statistics)\n",
        "# Hint: scaler = StandardScaler()\n",
        "#       X_train_scaled = scaler.fit_transform(X_train)\n",
        "#       X_val_scaled = scaler.transform(X_val)\n",
        "#       X_test_scaled = scaler.transform(X_test)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Convert to PyTorch tensors\n",
        "# Hint: X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "#       y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Create TensorDatasets\n",
        "# Hint: train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Create DataLoaders with batch_size=32\n",
        "# Hint: train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) # No need to shuffle validation data\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) # No need to shuffle test data\n",
        "\n",
        "\n",
        "\n",
        "# ‚úì CHECKPOINT: Verify shapes\n",
        "print(\"Data Preparation Complete!\")\n",
        "print(f\"Train set: {X_train_tensor.shape[0]} samples\")\n",
        "print(f\"Val set: {X_val_tensor.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test_tensor.shape[0]} samples\")\n",
        "print(f\"Features: {X_train_tensor.shape[1]}\")\n",
        "print(f\"Batches in train_loader: {len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiexsrMLYOMm"
      },
      "source": [
        "## üèóÔ∏è Milestone 2: Define the Neural Network (15 minutes)\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "Input Layer:    8 features\n",
        "Hidden Layer 1: 16 neurons + ReLU activation\n",
        "Hidden Layer 2: 8 neurons + ReLU activation\n",
        "Output Layer:   2 neurons (no diabetes / diabetes)\n",
        "```\n",
        "\n",
        "**Your Task:** Build this network using `nn.Module`\n",
        "\n",
        "**Key Concepts:**\n",
        "- `nn.Linear(in_features, out_features)` - Fully connected layer\n",
        "- `nn.ReLU()` - Activation function\n",
        "- `forward()` - Define how data flows through the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifPAsmPzYOMm"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# TODO: Define the neural network class\n",
        "class DiabetesClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesClassifier, self).__init__()\n",
        "\n",
        "        # TODO: Define layers\n",
        "        # Hint: self.fc1 = nn.Linear(8, 16)  # Input to hidden1\n",
        "        #       self.relu = nn.ReLU()\n",
        "        #       self.fc2 = nn.Linear(16, 8)  # Hidden1 to hidden2\n",
        "        #       self.fc3 = nn.Linear(8, 2)   # Hidden2 to output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement forward pass\n",
        "        # Hint: x = self.fc1(x)\n",
        "        #       x = self.relu(x)\n",
        "        #       x = self.fc2(x)\n",
        "        #       x = self.relu(x)\n",
        "        #       x = self.fc3(x)\n",
        "        #       return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        pass\n",
        "\n",
        "# TODO: Instantiate the model\n",
        "model = None  # DiabetesClassifier()\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Print model architecture\n",
        "# print(model)\n",
        "\n",
        "\n",
        "# ‚úì CHECKPOINT: Count parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "if model is not None:\n",
        "    print(f\"\\nTotal parameters: {count_parameters(model)}\")\n",
        "    print(\"Expected: ~300 parameters\")\n",
        "\n",
        "    # Test forward pass\n",
        "    test_input = torch.randn(1, 8)\n",
        "    test_output = model(test_input)\n",
        "    print(f\"\\nTest input shape: {test_input.shape}\")\n",
        "    print(f\"Test output shape: {test_output.shape}\")\n",
        "    print(\"Expected output shape: torch.Size([1, 2])\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55iTgII0YOMm"
      },
      "source": [
        "## ‚öôÔ∏è Milestone 3: Training Setup (10 minutes)\n",
        "\n",
        "**Components needed:**\n",
        "1. Loss function - `CrossEntropyLoss` (for classification)\n",
        "2. Optimizer - `Adam` (adaptive learning rate)\n",
        "3. Learning rate - Start with 0.001\n",
        "4. Device - GPU if available, CPU otherwise\n",
        "5. Number of epochs - 50\n",
        "\n",
        "**Why these choices?**\n",
        "- CrossEntropyLoss: Standard for multi-class classification\n",
        "- Adam: Generally works well, adapts learning rate automatically\n",
        "- lr=0.001: Good starting point, can tune later\n",
        "- 50 epochs: Enough to see convergence, not too slow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvc0gnoIYOMm"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# TODO: Define loss function\n",
        "# Hint: criterion = nn.CrossEntropyLoss()\n",
        "criterion = None\n",
        "\n",
        "\n",
        "# TODO: Define optimizer\n",
        "# Hint: optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer = None\n",
        "\n",
        "\n",
        "# TODO: Set device (GPU if available)\n",
        "# Hint: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = None\n",
        "\n",
        "\n",
        "# TODO: Move model to device\n",
        "# Hint: model = model.to(device)\n",
        "\n",
        "\n",
        "# Set number of epochs\n",
        "num_epochs = 50\n",
        "\n",
        "# ‚úì CHECKPOINT: Verify setup\n",
        "print(\"Training Setup Complete!\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Loss function: {criterion}\")\n",
        "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
        "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "print(f\"Number of epochs: {num_epochs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC5gCicLYOMm"
      },
      "source": [
        "## üîÑ Milestone 4: Training Loop - THE MOST IMPORTANT! (15 minutes)\n",
        "\n",
        "**This is where the learning happens!**\n",
        "\n",
        "For each epoch:\n",
        "1. **Training Phase:**\n",
        "   - Set model to training mode\n",
        "   - For each batch:\n",
        "     - Move batch to device\n",
        "     - Zero gradients (IMPORTANT!)\n",
        "     - Forward pass\n",
        "     - Compute loss\n",
        "     - Backward pass (compute gradients)\n",
        "     - Update weights\n",
        "\n",
        "2. **Validation Phase:**\n",
        "   - Set model to evaluation mode\n",
        "   - No gradient computation\n",
        "   - Compute validation loss and accuracy\n",
        "\n",
        "**This is the CORE of deep learning - understand every line!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbYG01WXYOMm"
      },
      "outputs": [],
      "source": [
        "# Lists to store metrics\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ============================================\n",
        "    # TRAINING PHASE\n",
        "    # ============================================\n",
        "\n",
        "    # TODO: Set model to training mode\n",
        "    # Hint: model.train()\n",
        "\n",
        "\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # TODO: Move batch to device\n",
        "        # Hint: batch_X = batch_X.to(device)\n",
        "        #       batch_y = batch_y.to(device)\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Zero the gradients (CRITICAL!)\n",
        "        # Hint: optimizer.zero_grad()\n",
        "        # Why? Gradients accumulate by default - we need to clear them each iteration\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Forward pass\n",
        "        # Hint: outputs = model(batch_X)\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Compute loss\n",
        "        # Hint: loss = criterion(outputs, batch_y)\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Backward pass (compute gradients)\n",
        "        # Hint: loss.backward()\n",
        "        # This is backpropagation!\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Update weights\n",
        "        # Hint: optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        # Accumulate loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # TODO: Calculate average training loss\n",
        "    # Hint: train_loss = train_loss / len(train_loader)\n",
        "\n",
        "\n",
        "\n",
        "    # ============================================\n",
        "    # VALIDATION PHASE\n",
        "    # ============================================\n",
        "\n",
        "    # TODO: Set model to evaluation mode\n",
        "    # Hint: model.eval()\n",
        "\n",
        "\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # TODO: Disable gradient computation (saves memory and computation)\n",
        "    # Hint: with torch.no_grad():\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            # TODO: Move batch to device\n",
        "\n",
        "\n",
        "\n",
        "            # TODO: Forward pass\n",
        "\n",
        "\n",
        "\n",
        "            # TODO: Compute loss\n",
        "\n",
        "\n",
        "\n",
        "            # Accumulate loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # TODO: Compute accuracy\n",
        "            # Hint: _, predicted = torch.max(outputs, 1)\n",
        "            #       total += batch_y.size(0)\n",
        "            #       correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: Calculate average validation loss and accuracy\n",
        "    # Hint: val_loss = val_loss / len(val_loader)\n",
        "    #       val_accuracy = 100 * correct / total\n",
        "\n",
        "\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print progress every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "        print(f\"  Val Accuracy: {val_accuracy:.2f}%\\n\")\n",
        "\n",
        "print(\"‚úì Training Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ62MAYfYOMn"
      },
      "source": [
        "### üìà Visualize Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjNaYcC4YOMn"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curves\n",
        "ax1.plot(train_losses, label='Training Loss', linewidth=2)\n",
        "ax1.plot(val_losses, label='Validation Loss', linewidth=2)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curve\n",
        "ax2.plot(val_accuracies, label='Validation Accuracy', color='green', linewidth=2)\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.set_title('Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracies[-1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsQjplNoYOMn"
      },
      "source": [
        "## üìä Milestone 5: Medical Evaluation Metrics (10 minutes)\n",
        "\n",
        "**In medical diagnosis, accuracy is NOT enough!**\n",
        "\n",
        "We need:\n",
        "- **Sensitivity (Recall)** - How many actual diabetics did we catch?\n",
        "- **Specificity** - How many non-diabetics did we correctly identify?\n",
        "- **Precision** - Of those we diagnosed as diabetic, how many actually are?\n",
        "- **F1-Score** - Harmonic mean of precision and recall\n",
        "\n",
        "**Why this matters:**\n",
        "- Missing a diabetic patient (False Negative) can be life-threatening\n",
        "- Falsely diagnosing diabetes (False Positive) causes unnecessary treatment\n",
        "- We need to balance these based on clinical priorities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbIdoi65YOMn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# TODO: Set model to evaluation mode\n",
        "# Hint: model.eval()\n",
        "\n",
        "\n",
        "# Get predictions on test set\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        # TODO: Move to device\n",
        "\n",
        "\n",
        "        # TODO: Get predictions\n",
        "        # Hint: outputs = model(batch_X)\n",
        "        #       _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# TODO: Compute metrics\n",
        "# Hint: accuracy = accuracy_score(all_labels, all_predictions)\n",
        "#       precision = precision_score(all_labels, all_predictions)\n",
        "#       sensitivity = recall_score(all_labels, all_predictions)  # Sensitivity = Recall\n",
        "#       f1 = f1_score(all_labels, all_predictions)\n",
        "\n",
        "accuracy = None\n",
        "precision = None\n",
        "sensitivity = None  # This is Recall\n",
        "f1 = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Compute specificity manually\n",
        "# Hint: cm = confusion_matrix(all_labels, all_predictions)\n",
        "#       tn, fp, fn, tp = cm.ravel()\n",
        "#       specificity = tn / (tn + fp)\n",
        "\n",
        "cm = None\n",
        "specificity = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MEDICAL EVALUATION METRICS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:    {accuracy*100:.2f}%\")\n",
        "print(f\"Precision:   {precision*100:.2f}%  (Of predicted diabetic, how many are correct?)\")\n",
        "print(f\"Sensitivity: {sensitivity*100:.2f}%  (Of actual diabetic, how many did we catch?)\")\n",
        "print(f\"Specificity: {specificity*100:.2f}%  (Of actual non-diabetic, how many correct?)\")\n",
        "print(f\"F1-Score:    {f1:.4f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Baseline comparison\n",
        "baseline_accuracy = max(np.mean(all_labels), 1 - np.mean(all_labels))\n",
        "print(f\"\\nBaseline (always predict majority class): {baseline_accuracy*100:.2f}%\")\n",
        "print(f\"Our model improvement: +{(accuracy - baseline_accuracy)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpY0gz79YOMn"
      },
      "source": [
        "### üéØ Confusion Matrix Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iqKiN6AYOMn"
      },
      "outputs": [],
      "source": [
        "# TODO: Create confusion matrix heatmap\n",
        "# Hint: Use seaborn heatmap\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Diabetes', 'Diabetes'],\n",
        "            yticklabels=['No Diabetes', 'Diabetes'])\n",
        "\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Confusion Matrix\\n')\n",
        "\n",
        "# Add interpretation\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "plt.text(0.5, -0.15, f\"True Negatives: {tn}\", transform=plt.gca().transAxes, ha='center')\n",
        "plt.text(0.5, -0.20, f\"False Positives: {fp} (unnecessary treatment)\", transform=plt.gca().transAxes, ha='center')\n",
        "plt.text(0.5, -0.25, f\"False Negatives: {fn} (missed diagnoses - CRITICAL!)\", transform=plt.gca().transAxes, ha='center', color='red')\n",
        "plt.text(0.5, -0.30, f\"True Positives: {tp}\", transform=plt.gca().transAxes, ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nClinical Interpretation:\")\n",
        "print(f\"- We correctly identified {tp} diabetic patients\")\n",
        "print(f\"- We MISSED {fn} diabetic patients (this is bad!)\")\n",
        "print(f\"- We falsely diagnosed {fp} healthy patients (unnecessary worry/treatment)\")\n",
        "print(f\"- We correctly identified {tn} healthy patients\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATHhpWGUYOMn"
      },
      "source": [
        "### üìã Detailed Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncxxgjj_YOMn"
      },
      "outputs": [],
      "source": [
        "# Print detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(all_labels, all_predictions,\n",
        "                          target_names=['No Diabetes', 'Diabetes']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH1kpTmsYOMn"
      },
      "source": [
        "## üî¨ Milestone 6: Experimentation & Optimization (Remaining time)\n",
        "\n",
        "**Now it's YOUR turn to improve the model!**\n",
        "\n",
        "Try different approaches and document what works:\n",
        "\n",
        "### Ideas to Experiment With:\n",
        "\n",
        "1. **Architecture Changes:**\n",
        "   - More/fewer layers\n",
        "   - Different layer sizes\n",
        "   - Add dropout for regularization\n",
        "   - Add batch normalization\n",
        "\n",
        "2. **Training Changes:**\n",
        "   - Different learning rates (0.0001, 0.01)\n",
        "   - Different optimizers (SGD, RMSprop)\n",
        "   - Different batch sizes (16, 64)\n",
        "   - More/fewer epochs\n",
        "\n",
        "3. **Data Changes:**\n",
        "   - Different train/val/test splits\n",
        "   - Different normalization\n",
        "   - Handle class imbalance (weighted loss)\n",
        "\n",
        "**Keep track of your experiments!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_2q_TPWYOMn"
      },
      "source": [
        "### Experiment 1: Deeper Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISWFE00LYOMn"
      },
      "outputs": [],
      "source": [
        "# TODO: Try a deeper network\n",
        "# Example: 8 ‚Üí 32 ‚Üí 16 ‚Üí 8 ‚Üí 2\n",
        "\n",
        "class DeeperClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeeperClassifier, self).__init__()\n",
        "        # Your architecture here\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your forward pass here\n",
        "        pass\n",
        "\n",
        "# Train and evaluate\n",
        "# (Copy training loop from above and modify)\n",
        "\n",
        "# Document results:\n",
        "# Accuracy: _____\n",
        "# Sensitivity: _____\n",
        "# Better than original? _____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-bZo9ExYOMn"
      },
      "source": [
        "### Experiment 2: Add Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPp0xtc1YOMo"
      },
      "outputs": [],
      "source": [
        "# TODO: Add dropout layers (e.g., 0.2 or 0.5)\n",
        "# Dropout helps prevent overfitting\n",
        "\n",
        "class DropoutClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DropoutClassifier, self).__init__()\n",
        "        # Add dropout: self.dropout = nn.Dropout(0.2)\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply dropout between layers\n",
        "        pass\n",
        "\n",
        "# Document results:\n",
        "# Accuracy: _____\n",
        "# Did it reduce overfitting? _____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rxrcDs8YOMo"
      },
      "source": [
        "### Experiment 3: Different Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3VnRuvuYOMo"
      },
      "outputs": [],
      "source": [
        "# TODO: Try lr=0.0001 (lower) or lr=0.01 (higher)\n",
        "\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Document results:\n",
        "# Learning rate: _____\n",
        "# Convergence speed: _____\n",
        "# Final accuracy: _____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvirruXtYOMo"
      },
      "source": [
        "### Experiment 4: Class Imbalance Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6l_S5ysYOMs"
      },
      "outputs": [],
      "source": [
        "# TODO: Use weighted loss to handle class imbalance\n",
        "# Medical datasets often have imbalanced classes\n",
        "\n",
        "# Calculate class weights\n",
        "# class_counts = np.bincount(y_train)\n",
        "# class_weights = 1. / class_counts\n",
        "# weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "# criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "# Document results:\n",
        "# Did sensitivity improve? _____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4voUEC3wYOMt"
      },
      "source": [
        "### üìä Experiment Comparison Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ1K3yrpYOMt"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a comparison table of all your experiments\n",
        "\n",
        "results = {\n",
        "    'Experiment': ['Baseline', 'Deeper', 'Dropout', 'Low LR', 'Weighted Loss'],\n",
        "    'Accuracy': [None, None, None, None, None],\n",
        "    'Sensitivity': [None, None, None, None, None],\n",
        "    'Specificity': [None, None, None, None, None],\n",
        "    'F1-Score': [None, None, None, None, None]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "# Which experiment performed best?\n",
        "# Your analysis:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XOqjFk_YOMt"
      },
      "source": [
        "---\n",
        "\n",
        "## üéì Reflection Questions\n",
        "\n",
        "Answer these questions based on your experiments:\n",
        "\n",
        "1. **What was the biggest challenge in building the neural network?**\n",
        "   - Your answer:\n",
        "\n",
        "2. **Which component of the training loop did you find most confusing initially?**\n",
        "   - Your answer:\n",
        "\n",
        "3. **Why is sensitivity (recall) particularly important for medical diagnosis?**\n",
        "   - Your answer:\n",
        "\n",
        "4. **What was your best performing model configuration?**\n",
        "   - Your answer:\n",
        "\n",
        "5. **If you were deploying this in a real hospital, what additional considerations would you have?**\n",
        "   - Your answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJWJv78NYOMt"
      },
      "source": [
        "## üéâ Lab 2 Complete!\n",
        "\n",
        "### What You Accomplished:\n",
        "\n",
        "‚úÖ Mastered PyTorch tensor operations  \n",
        "‚úÖ Built a complete neural network from scratch  \n",
        "‚úÖ Implemented the full training loop  \n",
        "‚úÖ Evaluated with medical metrics  \n",
        "‚úÖ Experimented with improvements  \n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "- **The training loop is fundamental** - forward, loss, backward, optimize\n",
        "- **Medical metrics matter** - accuracy alone is insufficient\n",
        "- **Experimentation is essential** - no \"perfect\" hyperparameters\n",
        "- **Neural networks are powerful** but require careful tuning\n",
        "\n",
        "**Good work! üéì‚ú®**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}